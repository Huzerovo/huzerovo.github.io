
<!DOCTYPE html>
<html  id="root" lang="zh-CN ">

<head>
    <meta charset="UTF-8" />
    <meta name="author" content="狐zerOAO" />
    <meta name="description" content="记录一段回忆" />
    <meta name="generator" content="记录一段回忆" />
    <meta name="keywords" content="blog,code" />
    <meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
    <meta name="theme-color" media="(prefers-color-scheme: dark)" content="dark">
    <meta name="viewport" content="width=device-width,initial-scale=1"/>

    
    
    <title> 线性回归 - 狐言狐语 | 记录一段回忆 </title>
    

    
    
<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/bootstrap-icons/1.10.4/font/bootstrap-icons.min.css">


    
    
<script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.6.4/jquery.min.js"></script>


    
    
        
<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.16/fancybox/fancybox.min.css">

        
<script src="https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.16/fancybox/fancybox.umd.min.js"></script>

    

    
    
        
<link rel="stylesheet" href="/css/github.min.css" id="code-style">

        
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>

        <script>hljs.highlightAll();</script>
    

    
    
<link rel="stylesheet" href="/css/huzerovo.css">


    
    
<link rel="stylesheet" href="/css/light/light.css">

    
<link rel="stylesheet" href="/css/dark/dark.css">


    
    
<script src="/js/huzerovo.js"></script>
<script src="/js/search.js"></script>


    
    
        
<script>
    MathJax = {
        tex: {
            tags: 'ams',
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
            fontCache: 'global'
        },
        chtml: {
            scale: 1.1
        }
    };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    
    <script>
        let codeStyle = {};
        codeStyle.light = "/css/github.min.css";
        codeStyle.dark = "/css/github-dark.min.css";
        window.load = myLoading();
        window.matchMedia("(prefers-color-scheme: light)").addEventListener('change', followTheme);
    </script>
</head>

<body>
    












<header class="topbar">
    <div class="topbar-container">
        
        <nav class="topbar-menu">
            <ul>
            
                <li>
                
                
                    <a href="/" title="首页">首页</a>
                
                </li>
            
                <li>
                
                
                    <a href="/archive" title="归档">归档</a>
                
                </li>
            
                <li>
                
                
                    <a href="/categories" title="分类">分类</a>
                
                </li>
            
                <li>
                
                
                    <a href="/tagcloud" title="标签云">标签云</a>
                
                </li>
            
                <li>
                
                
                    <a href="/about" title="关于我">关于我</a>
                
                </li>
            
            </ul>
        </nav>

        
        <div class="topbar-icon-buttons">
            <button id="btn-switch-theme" class="icon-button" title="切换主题">
                <i class="bi bi-sun"></i>
            </button>
            
            <a id="btn-search" class="icon-button" title="搜索" href="/search">
                <i class="bi bi-search"></i>
            </a>
            
        </div>
    </div>
</header>

<main class="container">
    
    <div id="article-toc">
    
        <p class="sidebar-list-title">目录</p>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80"><span class="toc-text">基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-%E2%80%9C%E7%89%B9%E5%BE%81%E2%80%9D-%E4%B8%8E-%E2%80%9C%E6%A0%87%E7%AD%BE%E2%80%9D"><span class="toc-text">定义 “特征” 与 “标签”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-%E2%80%9C%E7%89%B9%E5%BE%81-%E6%A0%87%E7%AD%BE%E2%80%9D-%E5%85%B3%E7%B3%BB"><span class="toc-text">定义 “特征-标签” 关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E5%87%BA%E9%97%AE%E9%A2%98"><span class="toc-text">提出问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%95%B0%E5%AD%A6%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0%E9%97%AE%E9%A2%98"><span class="toc-text">使用数学语言描述问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E5%90%91%E9%87%8F%E8%BF%9B%E8%A1%8C%E8%A1%A8%E8%BE%BE"><span class="toc-text">引入向量进行表达</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98"><span class="toc-text">进一步分析问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E8%AE%AD%E7%BB%83%E9%9B%86%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-text">引入数据集与训练集的概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E8%BD%AC%E5%8C%96%E9%97%AE%E9%A2%98"><span class="toc-text">引入最小二乘法转化问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="toc-text">为什么使用最小二乘法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98"><span class="toc-text">解决问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="toc-text">牛顿法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90%E8%A7%A3%E6%B3%95"><span class="toc-text">解析解法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89"><span class="toc-text">符号定义</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E7%9F%A9%E9%98%B5%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E5%87%BD%E6%95%B0"><span class="toc-text">定义矩阵函数的导函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-trace-%E8%BF%90%E7%AE%97"><span class="toc-text">定义 trace 运算</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E5%AE%9A%E7%90%86"><span class="toc-text">一些定理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="toc-text">解析式推导</span></a></li></ol></li></ol></li></ol>
    
    </div>
    
    <article class="article-container">
        <h1>线性回归</h1>
        <blockquote>
<p>基于 Andrew NG 机器学习 公开课程 <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup><br />
线性回归是一种 <strong>监督学习 (supervised leaning)</strong> 的模型<br />
简单解释其中的数学原理，方便回顾</p>
</blockquote>
<span id="more"></span>
<h2 id="%E5%9F%BA%E7%A1%80" tabindex="-1" id="基础"><a class="header-anchor" href="#基础"># </a>基础</h2>
<h3 id="%E5%AE%9A%E4%B9%89-%E2%80%9C%E7%89%B9%E5%BE%81%E2%80%9D-%E4%B8%8E-%E2%80%9C%E6%A0%87%E7%AD%BE%E2%80%9D" tabindex="-1" id="定义-“特征”-与-“标签”"><a class="header-anchor" href="#定义-“特征”-与-“标签”"># </a>定义 “特征” 与 “标签”</h3>
<p>这里的定义是为了方便后面的表达</p>
<p>将监督学习中的一些概念 “重命名”</p>
<ul>
<li>特征，定义为输入空间（特征空间）</li>
<li>标签，定义为输出空间</li>
</ul>
<p>对 “特征” 与 “标签” 的理解</p>
<ol>
<li>
<p>从算法定义上</p>
<ul>
<li>特征可理解为输入</li>
<li>标签可理解为输出</li>
<li>算法的目的就是根据输入，给出输出</li>
</ul>
</li>
<li>
<p>从直观感觉上</p>
<ul>
<li>特征指代一个对象的特点</li>
<li>标签指代一个对象</li>
<li>算法的目的即根据特点，猜测对象</li>
</ul>
</li>
</ol>
<h3 id="%E5%AE%9A%E4%B9%89-%E2%80%9C%E7%89%B9%E5%BE%81-%E6%A0%87%E7%AD%BE%E2%80%9D-%E5%85%B3%E7%B3%BB" tabindex="-1" id="定义-“特征-标签”-关系"><a class="header-anchor" href="#定义-“特征-标签”-关系"># </a>定义 “特征-标签” 关系</h3>
<p>实际生活中，我们知道了 $A, B$ 能够在某种程度上决定 $C$ ，
且 $A, B$ 各自对 $C$ 的影响有多大，我们并不清楚</p>
<p>也就是说，我们知道 “特征” 与 “标签” 之间存在某种关系，但无法准确地描述这个关系</p>
<p>此时我们可以对 $A, B, C$ 建立一个关系，即：能由 $A, B$ 推出 $C$<br />
使用数学符号表示： $(A, B) \Rightarrow C$</p>
<p>若将 $A$ 和 $B$ 称做 $C$ 的特征，并且将 $C$ 定义为标签，
将 $(A, B) \Rightarrow C$ 称为 “特征-标签” 关系，
则称数据 $(A, B, C)$ 存在 “特征-标签” 关系</p>
<h3 id="%E6%8F%90%E5%87%BA%E9%97%AE%E9%A2%98" tabindex="-1" id="提出问题"><a class="header-anchor" href="#提出问题"># </a>提出问题</h3>
<p>我们希望，给出一组存在 “特征-标签” 的关系的数据，
计算机能够根据这组数据，建立一个存在 “特征-标签” 关系的模型<br />
随后给出 “特征” ，计算机能够根据模型，给出比较可靠的、推测的 “标签”</p>
<h3 id="%E4%BD%BF%E7%94%A8%E6%95%B0%E5%AD%A6%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0%E9%97%AE%E9%A2%98" tabindex="-1" id="使用数学语言描述问题"><a class="header-anchor" href="#使用数学语言描述问题"># </a>使用数学语言描述问题</h3>
<p>接下来将问题抽象成为一个数学问题</p>
<p>设所有的特征的集合为 $(x_{1}, x_{2}, \cdots, x_{n})$，其中，$n$ 表示特征的数量，
$y$ 为特征对应的 “标签”</p>
<p>在比较简单的情况下，假定 “特征-标签” 是线性关系，并且这个线性关系为函数 $h$ ，
则称 $h$ 为 <strong>假设函数 (hypotheses)</strong></p>
<p>于是得到的函数</p>
<p>$$
\begin{equation}
h_{\theta}(x) = \theta_{0} + \theta_{1} \cdot x_{1} +
\theta_{2} \cdot x_{2} + \cdots + \theta_{n} \cdot x_{n}
\end{equation}
$$</p>
<p>其中， $\theta_{0}, \theta_{1}, \cdots, \theta_{n}$
表示 <strong>参数 (parameters)</strong> ，或者说是 <strong>权值 (weights)</strong>
这是我们需要求得的值</p>
<h3 id="%E5%BC%95%E5%85%A5%E5%90%91%E9%87%8F%E8%BF%9B%E8%A1%8C%E8%A1%A8%E8%BE%BE" tabindex="-1" id="引入向量进行表达"><a class="header-anchor" href="#引入向量进行表达"># </a>引入向量进行表达</h3>
<p>若用向量表示 $\theta$ ：</p>
<p>$$
\vec{\theta} =
\begin{pmatrix}
\theta_{0}\\
\theta_{1}\\
\vdots\\
\theta_{n}
\end{pmatrix}
$$</p>
<p>并将特征集合重新定义为特征向量：</p>
<p>$$
\vec{x} =
\begin{pmatrix}
x_{0}\\
x_{1}\\
\vdots\\
x_{n}
\end{pmatrix}
$$</p>
<p>其中 $x_{0} = 1$，
<strong>为简化符号，记 $\vec{\theta}$ 为 $\theta$ ，$\vec{x}$ 为 $x$</strong> ，
则函数可表示为：</p>
<p>$$
\begin{equation}
h_{\theta}(x) = \sum_{i=0}^{n}{\theta_{i} x_{i}} = \theta^{T}x
\end{equation}
$$</p>
<p>此时我们的问题就是，求出一个 $\theta$ ，使得函数 $h$ 能够很好地描述 “特征-标签” 关系，
即建立一个合理的模型</p>
<h2 id="%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98" tabindex="-1" id="进一步分析问题"><a class="header-anchor" href="#进一步分析问题"># </a>进一步分析问题</h2>
<h3 id="%E5%BC%95%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E8%AE%AD%E7%BB%83%E9%9B%86%E7%9A%84%E6%A6%82%E5%BF%B5" tabindex="-1" id="引入数据集与训练集的概念"><a class="header-anchor" href="#引入数据集与训练集的概念"># </a>引入数据集与训练集的概念</h3>
<p>上面的函数仅表示单个数据中的 “特征与标签” 关系<br />
若存在多个数据，即 <strong>数据集 (dataset)</strong> ，
且所有的数据都存在 “特征-标签” 的关系，
即可以构建出一个 <strong>训练集 (training set)</strong><br />
那么我们可以使用 <strong>训练集</strong> 对算法进行 <strong>训练</strong> ，最终得出 $\theta$</p>
<p>定义下面的符号，方便之后的表示：<br />
设 <strong>训练集大小</strong> 为 $m$ ，令 $x_{i}^{(j)}$ 表示训练集中，
<strong>第 $j$ 项数据</strong> 的 <strong>第 $i$ 个特征</strong>，
$x^{(j)}$ 表示训练集中， <strong>第 $j$ 项数据</strong> 的特征向量<br />
同理可得 $\theta_{i}^{(j)}$ 与 $\theta^{(j)}$</p>
<h3 id="%E5%BC%95%E5%85%A5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E8%BD%AC%E5%8C%96%E9%97%AE%E9%A2%98" tabindex="-1" id="引入最小二乘法转化问题"><a class="header-anchor" href="#引入最小二乘法转化问题"># </a>引入最小二乘法转化问题</h3>
<p>回顾我们的问题：<br />
我们希望得到一个函数 $h$ 能够很好地描述训练集中的每个数据中 “特征-标签” 的关系<br />
换句话说，给出 “特征” ，若 “特征” 存在于训练集中，那么
由 <strong>函数 $h$</strong> 得出的 “标签”，与训练集中的 “标签”，它们之间的误差最小<br />
根据 <strong>最小二乘法</strong> ，得：</p>
<p>$$
\begin{equation}
J(\theta) = \frac{1}{2} \sum_{i=1}^{m} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^{2}
\end{equation}
$$</p>
<p>函数 $J$ 也称为 <strong>cost function</strong></p>
<p>于是问题转化为：<br />
需要得到一个 $\theta$ ，使得 $J(\theta)$ 的值最小</p>
<h3 id="%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95" tabindex="-1" id="为什么使用最小二乘法"><a class="header-anchor" href="#为什么使用最小二乘法"># </a>为什么使用最小二乘法</h3>
<p>参考文章 <a href="/2021/11/03/why-use-least-squares/" title="为什么使用最小二乘法">为什么使用最小二乘法</a></p>
<h2 id="%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98" tabindex="-1" id="解决问题"><a class="header-anchor" href="#解决问题"># </a>解决问题</h2>
<p>接下来介绍解决这个问题的可行方法</p>
<h3 id="%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D" tabindex="-1" id="梯度下降"><a class="header-anchor" href="#梯度下降"># </a>梯度下降</h3>
<blockquote>
<p>注：梯度下降算法在 <a href="/2021/09/15/gradient-descent/" title="另一篇文章">另一篇文章</a> 中有介绍</p>
</blockquote>
<p>为了得到 $\theta$ 使函数值 $J(\theta)$ 最小，可以采取 <strong>梯度下降 (gradient descent)</strong> 的方法：<br />
随机选取一个 $\theta$ 值，随后不断地更新 $\theta$ ，使 $J(\theta)$ 不断减小，
当 $J(\theta)$ 的值达到最小时，得到的 $\theta$ 即是问题的解</p>
<p>我们可以这样进行更新：
$$<br />
\begin{equation}
\theta_{j} := \theta_{j} - \alpha \frac {\partial}{\partial \theta_{j}} J(\theta)
\label{eq4}
\end{equation}
$$</p>
<p>其中 $:=$ 表示赋值，将右边的值赋予左边，而 $=$ 是真值判断，表示等号两边的值或结果是相同的<br />
$\alpha$ 表示 <strong>学习率 (learning rate)</strong> ，或者称 <strong>步长</strong> ，是自定义的常数</p>
<p>对 $J(\theta)$ 进行偏导：
$$
\begin{equation}
\begin{split}
\frac{\partial}{\partial \theta_{j}} J(\theta)
&amp;= \frac{\partial}{\partial \theta_{j}} \frac {1}{2} (h_{\theta}(x) - y)^2 \\
&amp;= 2 \cdot \frac{1}{2}(h_{\theta}(x) - y) \cdot
\frac{\partial}{\partial \theta_{j}} (h_{\theta}(x) - y) \\
&amp;= (h_{\theta}(x) - y) \cdot \frac{\partial}{\partial \theta_{j}}
\left( \sum_{i=0}^{n} \theta_{i} x_{i} - y \right) \\
&amp;= (h_{\theta}(x) - y) x_{j}
\end{split}
\end{equation}
$$</p>
<p>代入 $\eqref{eq4}$ ，得：</p>
<p>$$
\begin{equation}
\theta_{j} := \theta_{j} - \alpha (h_{\theta}(x) - y) x_{j}
\end{equation}
$$</p>
<p>这是单个数据的表示，引入上面介绍的训练集</p>
<p>$$
\begin{equation}
\theta_{j} := \theta_{j} - \alpha \sum_{i=1}^{m} \left(h_{\theta}(x^{(i)})
- y^{(i)}\right) x_{j}^{(i)}
\end{equation}
$$</p>
<p>对每一个在向量 $\theta$ 中的分量 $\theta_{j}$ 都进行上述运算<br />
最终我们会得到一个 $\theta$ ，这个值就是我们希望求得的值</p>
<h3 id="%E7%89%9B%E9%A1%BF%E6%B3%95" tabindex="-1" id="牛顿法"><a class="header-anchor" href="#牛顿法"># </a>牛顿法</h3>
<blockquote>
<p>尚未完成，咕咕咕ing</p>
</blockquote>
<p>因为极值点处的一阶导数为 0 ，
且 $J(\theta)$ 的函数最高次为二次，函数只存在一个极值点<br />
所以，为了得到 $J(\theta)$ 的最小值，我们可以使用间接的方法，
即：寻找 $J(\theta)$ 导数的零点</p>
<p>牛顿法的步骤为：</p>
<blockquote>
<p>设函数为 $f(x)$</p>
<ol>
<li>随机在函数 $f(x)$ 上取一初始点 $P_{0}$</li>
<li>过点 $P_{0}$ 作函数 $f(x)$ 的切线 $L$</li>
<li>找到切线 $L$ 与 $x$ 轴的交点 $Q_{0}$</li>
<li>过交点 $Q_{0}$ 作 $x$ 轴的垂线，并与函数有一交点 $P_{1}$</li>
<li>将点 $P_{1}$ 作为初始点，回到步骤 2</li>
<li>当点 $P_{n}$ 与点 $Q_{n}$ 重合时，横坐标即零点</li>
</ol>
</blockquote>
<h3 id="%E8%A7%A3%E6%9E%90%E8%A7%A3%E6%B3%95" tabindex="-1" id="解析解法"><a class="header-anchor" href="#解析解法"># </a>解析解法</h3>
<p>实际上，上面的两种方法得到的是一个近似值，
但其精确度能够满足实际应用，
而解析解法能够直接计算出精确值</p>
<h4 id="%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89" tabindex="-1" id="符号定义"><a class="header-anchor" href="#符号定义"># </a>符号定义</h4>
<blockquote>
<p>这里的定义均来自 Andrew NG 课程讲义</p>
</blockquote>
<h5 id="%E5%AE%9A%E4%B9%89%E7%9F%A9%E9%98%B5%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E5%87%BD%E6%95%B0" tabindex="-1" id="定义矩阵函数的导函数"><a class="header-anchor" href="#定义矩阵函数的导函数"># </a>定义矩阵函数的导函数</h5>
<p>对于函数 $f$ : $\mathbb{R}^{m \times n} \mapsto \mathbb{R}$ ，
即 $m \times n$ 的矩阵映射至实数的函数关系<br />
定义函数 $f$ 对矩阵 $A$ 的导数为：</p>
<p>$$
\begin{equation}
\nabla_{A}f(A) =
\begin{bmatrix}
\frac{\partial{f}}{\partial A_{11}} &amp; \cdots &amp;
\frac{\partial{f}}{\partial A_{1n}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial{f}}{\partial A_{m1}} &amp; \cdots &amp;
\frac{\partial{f}}{\partial A_{mn}} \\
\end{bmatrix}
\end{equation}
$$</p>
<p><em>例子</em><br />
若矩阵
$
A =
\begin{bmatrix}
A_{11} &amp; A_{12} \\
A_{21} &amp; A_{22}
\end{bmatrix}
$ ，
函数 $f(A) = \frac{3}{2} A_{11} + 5A_{12}^{2} + A_{21}A_{22}$ ，
那么
$
\nabla_{A} f(A) =
\begin{bmatrix}
\frac{3}{2} &amp; 10A_{12} \\
A_{22} &amp; A_{21}
\end{bmatrix}
$</p>
<h5 id="%E5%AE%9A%E4%B9%89-trace-%E8%BF%90%E7%AE%97" tabindex="-1" id="定义-trace-运算"><a class="header-anchor" href="#定义-trace-运算"># </a>定义 trace 运算</h5>
<p><code>trace</code> 运算简写为 <code>tr</code><br />
对于 $n \times n$ 的矩阵 $A$ ，定义</p>
<p>$$
\begin{equation}
\mathrm{tr}A = \sum_{i=1}^{n}A_{ii}
\end{equation}
$$</p>
<p>可以知道， <code>trace</code> 运算具有下列运算法则</p>
<p>$$
\begin{equation}
\label{algorithm1}
\mathrm{tr}ABC = \mathrm{tr}CAB = \mathrm{tr}BCA
\end{equation}
$$</p>
<p>$$
\begin{equation}
\label{algorithm2}
\mathrm{tr}(A+B) = \mathrm{tr}A + \mathrm{tr}B
\end{equation}
$$</p>
<p>$$
\begin{equation}
\label{algorithm3}
\mathrm{tr}aA = a\mathrm{tr}A
\end{equation}
$$
其中 $a$ 是一个实数</p>
<h4 id="%E4%B8%80%E4%BA%9B%E5%AE%9A%E7%90%86" tabindex="-1" id="一些定理"><a class="header-anchor" href="#一些定理"># </a>一些定理</h4>
<p>一些 <strong>trace 运算</strong> 与 <strong>矩阵导函数</strong> 相关的定理<br />
仅列出 <strong>解析式推导过程</strong> 需要用到的一些定理，不作证明</p>
<p>$$
\begin{equation}
\label{theorem1}
\nabla_{A} \mathrm{tr}AB = B^{T}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\label{theorem2}
\nabla_{A^{T}} f(A) = (\nabla_{A}f(A))^{T}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\label{theorem3}
\nabla_{A} \mathrm{tr} A B A^{T} C = C A B + C^{T} A B^{T}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\label{theorem4}
\nabla_{A} |A| = |A| ( A^{-1} )^{T}
\end{equation}
$$</p>
<h4 id="%E8%A7%A3%E6%9E%90%E5%BC%8F%E6%8E%A8%E5%AF%BC" tabindex="-1" id="解析式推导"><a class="header-anchor" href="#解析式推导"># </a>解析式推导</h4>
<p>使用矩阵表达 “特征” 与 “标签”<br />
定义矩阵 $X$ 和 $Y$</p>
<p>$$
\begin{equation}
X =
\begin{bmatrix}
( x^{(1)} )^{T} \\
( x^{(2)} )^{T} \\
\vdots \\
( x^{(m)} )^{T}
\end{bmatrix}
,
Y =
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)}
\end{bmatrix}
\end{equation}
$$</p>
<blockquote>
<p>注意，此处 $x$ 表示向量 $\vec{x}$ ， $y$ 表示特征</p>
</blockquote>
<p>因为 $h_{\theta}(x^{(i)}) = \theta^{T} x^{(i)} = ( x^{(i)} )^{T} \theta$ ，故</p>
<p>$$
\begin{equation}
h_{\theta}(x)=
\begin{bmatrix}
( x^{(1)} )^{T} \theta \\
( x^{(2)} )^{T} \theta \\
\vdots \\
( x^{(m)} )^{T} \theta
\end{bmatrix}
= X \theta
\end{equation}
$$</p>
<p>又因为</p>
<p>$$
\begin{equation}
X \theta - Y =
\begin{bmatrix}
( x^{(1)} )^{T} \theta \\
( x^{(2)} )^{T} \theta \\
\vdots \\
( x^{(m)} )^{T} \theta
\end{bmatrix} -
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)}
\end{bmatrix} =
\begin{bmatrix}
h_{\theta}(x^{(1)}) - y^{(1)} \\
h_{\theta}(x^{(2)}) - y^{(2)} \\
\vdots \\
h_{\theta}(x^{(m)}) - y^{(m)}
\end{bmatrix}
\end{equation}
$$</p>
<p>且对于矩阵 $z$ ，有 $z^{T}z = \sum_{i} z_{i}^{2}$ ，
于是可得</p>
<p>$$
\begin{equation}
\begin{split}
\frac{1}{2} (X \theta - Y)^{T} (X \theta - Y)
&amp;= \frac{1}{2} \sum_{i = 1}^{m} ( h_{\theta}(x^{(i)}) - y^{(i)} )^{2} \\
&amp;= J(\theta)
\end{split}
\end{equation}
$$</p>
<p>根据 $\eqref{theorem2}$ 和 $\eqref{theorem3}$ ，有</p>
<p>$$
\begin{equation}
\label{inference1}
\nabla_{A^{T}} \mathrm{tr} AB A^{T} C = B^{T} A^{T} C^{T} + B A^{T} C
\end{equation}
$$</p>
<p>于是</p>
<p>$$
\begin{equation}
\begin{split}
\nabla_{\theta} J(\theta)
&amp;= \nabla_{\theta} \frac{1}{2} ( X \theta - Y )^{T} (X \theta - Y) \\
&amp;= \frac{1}{2} \nabla_{\theta} ( \theta^{T} X^{T} X \theta -
\theta^{T} X^{T} Y - Y^{T} X \theta + Y^{T} Y ) \\
&amp;= \frac{1}{2} \nabla_{\theta} \mathrm{tr} ( \theta^{T} X^{T} X \theta -
\theta^{T} X^{T} Y - Y^{T} X \theta + Y^{T} Y ) \\
&amp;= \frac{1}{2} \nabla_{\theta} ( \mathrm{tr} \theta^{T} X^{T} X \theta -
2 \mathrm{tr} Y^{T} X \theta ) \\
&amp;= \frac{1}{2} ( X^{T} X \theta + X^{T} X \theta - 2 X^{T} Y ) \\
&amp;= X^{T}  X \theta - X^{T} Y
\end{split}
\end{equation}
$$</p>
<p>第三步将
$(\theta^{T} X^{T} X \theta - \theta^{T} X^{T} Y - Y^{T} X \theta +
Y^{T} Y )$
视作整体，
可知这是一个实数，可将其看作一个
$1 \times 1$ 的矩阵 $A = \begin{bmatrix}a\end{bmatrix}$ ，
由 <code>trace</code> 定义可知 $\mathrm{tr} A = a$</p>
<p>第四步使用了 $\mathrm{tr} A = \mathrm{tr} A^{T}$ ，
且由于 $Y^{T}Y$ 项不存在 $\theta$ ，求导后为 $0$</p>
<p>第五步中使用了 $\eqref{theorem1}$ 和 $\eqref{inference1}$ ，
视 $A^{T} = \theta$ ， $B = B^{T} = X^{T} X$ , $C = I$</p>
<p>使 $J(\theta)$ 最小，则令 $\nabla_{\theta} J(\theta) = 0$ ，
所以有 $X^{T}  X \theta = X^{T} Y$<br />
因此，得 $\theta = ( X^{T}X )^{-1} X^{T} Y$</p>
<hr class="footnotes-sep" />
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>课程链接 <a target="_blank" rel="noopener" href="https://see.stanford.edu/Course/CS229">https://see.stanford.edu/Course/CS229</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

        
        <div class="article-meta-info">
            
                <span>分类</span>
                <a class="article-categories-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                
            
                <span>标签</span>
                <a class="article-tags-none-link" href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">监督学习</a><a class="article-tags-none-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a>
            
        </div>
        
        <div class="article-date-info">
            <span>发布日期: 2021-09-10</span>
            <span>更新日期: 2021-11-03</span>
        </div>
        <div class="article-comment">
            

    
    
    
    
    
    
    <div id="gitalk-container"></div>
    
<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/gitalk/1.8.0/gitalk.css">

    
<script src="https://cdn.bootcdn.net/ajax/libs/gitalk/1.8.0/gitalk.js"></script>

    
<script src="https://cdn.bootcdn.net/ajax/libs/blueimp-md5/2.19.0/js/md5.min.js"></script>

    <script>
        let c = {"enable":true,"clientID":"8430fc1311285afb9029","clientSecret":"6d9f1ff1d6db0df95fd537a3f3b2c686d35d641a","repo":"huzerovo.github.io","owner":"HuzerOVO","admin":["HuzerOVO"],"distractionFreeMode":false};
        c.title = "Article: " + "线性回归";
        c.id = md5(c.title);
        c.number = -1;
        let gitalk = new Gitalk(c);
        gitalk.render("gitalk-container");
    </script>


        </div>
    </article>
</main>

    
    <button id="goto-top" type="button" title="回到顶部">
        回到顶部
    </button>
    <footer>
        <p> Copyright 2023 @狐zerOAO</p>
        <p>
            Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>. 
            Theme by <a target="_blank" rel="noopener" href="https://github.com/HuzerOVO/hexo-theme-huzerovo">狐zerOAO</a>.
        </p>
        <br>
    </footer>
</body>

<script>
$('#goto-top').hide();
// 页面载入后设置icon
setTheme(getStorage('currentTheme'));
$(document).ready(function() {
    // 监听事件
    $('#btn-switch-theme').click(switchTheme);
    $('#btn-side-menu').click(function() {
        let e = $('#side-nav');
        let hide = 'side-menu-hidden'
        let show = 'side-menu-show'
        if (e.hasClass(hide)) {
            e.removeClass(hide);
            e.addClass(show);
        } else {
            e.removeClass(show);
            e.addClass(hide);
        }
    });
    $('#search').on('input', function() {
        searchArticle("/search.xml");
    });
    $('#goto-top').click(function() {
        $('#root').scrollTop(0);
    });

    $(window).scroll(function() {
        if ($('#root').scrollTop() > 300) {
            $('#goto-top').show();
        } else {
            $('#goto-top').hide();
        }
    });

    Fancybox.bind(".container img");
});
</script>
</html>
