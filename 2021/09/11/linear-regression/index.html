<!doctype html>
<html id="root" lang="zh-CN">
  <head>
    <meta charset="UTF-8" />
    <meta name="author" content="狐zerOAO" />
    <meta name="description" content="记录一段回忆" />
    <meta name="keywords" content="blog,code" />
    <meta
      name="theme-color"
      media="(prefers-color-scheme: light)"
      content="white"
    />
    <meta
      name="theme-color"
      media="(prefers-color-scheme: dark)"
      content="dark"
    />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>线性回归 - 狐言狐语 | 记录一段回忆</title>
    <meta name="generator" content="Hexo 7.3.0" />
    <meta
      name="description"
      content="基于Andrew NG机器学习公开课程[1]。 线性回归是一种监督学习的模型。"
    />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="线性回归" />
    <meta
      property="og:url"
      content="https://huzerovo.github.io/2021/09/11/linear-regression/"
    />
    <meta property="og:site_name" content="狐灵" />
    <meta
      property="og:description"
      content="基于Andrew NG机器学习公开课程[1]。 线性回归是一种监督学习的模型。"
    />
    <meta property="og:locale" content="zh_CN" />
    <meta
      property="article:published_time"
      content="2021-09-11T04:00:00.000Z"
    />
    <meta property="article:modified_time" content="2023-07-21T04:00:00.000Z" />
    <meta property="article:author" content="狐zerOAO" />
    <meta property="article:tag" content="监督学习" />
    <meta property="article:tag" content="线性回归" />
    <meta name="twitter:card" content="summary" />
    <link
      rel="alternate"
      href="/atom.xml"
      title="狐灵"
      type="application/atom+xml"
    />
    <script src="/js/jquery-3.7.1.min.js"></script>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.css"
    />
    <script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.umd.js"></script>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css"
    />
    <link rel="stylesheet" href="/css/github.min.css" id="code-style" />
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/highlight.min.js"></script>
    <link rel="stylesheet" href="/css/light.css" />
    <link rel="stylesheet" href="/css/dark.css" />
    <link rel="stylesheet" href="/css/huzerovo.css" />
    <script src="/js/huzerovo.js"></script>
    <script src="/js/search.js"></script>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css"
    />
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          // customised options
          // • auto-render specific keys, e.g.:
          delimiters: [
            { left: "$$", right: "$$", display: true },
            { left: "$", right: "$", display: false },
            { left: "\\(", right: "\\)", display: false },
            { left: "\\[", right: "\\]", display: true },
          ],
          // • rendering keys, e.g.:
          throwOnError: false,
        });
      });
    </script>
    <script>
      const ThemeConf = {
        rootId: "root",
        codeId: "code-style",
        ibtnId: "btn-switch-theme",
        iconDarkClass: "bi-moon",
        iconLightClass: "bi-sun",
        codeStyle: {
          light: "/css/github.min.css",
          dark: "/css/github-dark.min.css",
        },
      };
      window.load = ThemeSwitcher.init();
      window
        .matchMedia("(prefers-color-scheme: light)")
        .addEventListener("change", ThemeSwitcher.follow);
    </script>
  </head>
  <body>
    <header id="topbar">
      <div class="topbar-container">
        <nav class="topbar-menu">
          <ul>
            <li>
              <a href="/" title="首页">首页</a>
            </li>
            <li>
              <a href="/archive" title="归档">归档</a>
            </li>
            <li>
              <a href="/categories" title="分类">分类</a>
            </li>
            <li>
              <a href="/tagcloud" title="标签云">标签云</a>
            </li>
            <li>
              <a href="/about" title="关于我">关于我</a>
            </li>
          </ul>
        </nav>
        <div class="topbar-icon-buttons">
          <a
            class="icon-button"
            title="Github"
            href="https://github.com/Huzerovo"
            ><i class="bi bi-github"></i
          ></a>
          <a class="icon-button" id="btn-search" title="搜索" href="/search"
            ><i class="bi bi-search"></i
          ></a>
          <a class="icon-button" id="btn-switch-theme" title="切换主题"
            ><i class="bi bi-sun"></i
          ></a>
        </div>
      </div>
    </header>
    <main class="container">
      <div id="article-toc">
        <p class="sidebar-list-title">目录</p>
        <ol class="toc">
          <li class="toc-item toc-level-2">
            <a class="toc-link" href="#%E5%9F%BA%E7%A1%80"
              ><span class="toc-text">基础</span></a
            >
            <ol class="toc-child">
              <li class="toc-item toc-level-3">
                <a
                  class="toc-link"
                  href="#%E5%AE%9A%E4%B9%89%E2%80%9C%E7%89%B9%E5%BE%81%E2%80%9D%E4%B8%8E%E2%80%9C%E6%A0%87%E7%AD%BE%E2%80%9D"
                  ><span class="toc-text">定义“特征”与“标签”</span></a
                >
              </li>
              <li class="toc-item toc-level-3">
                <a
                  class="toc-link"
                  href="#%E5%AE%9A%E4%B9%89%E2%80%9C%E7%89%B9%E5%BE%81-%E6%A0%87%E7%AD%BE%E2%80%9D%E5%85%B3%E7%B3%BB"
                  ><span class="toc-text">定义“特征-标签”关系</span></a
                >
              </li>
              <li class="toc-item toc-level-3">
                <a class="toc-link" href="#%E6%8F%90%E5%87%BA%E9%97%AE%E9%A2%98"
                  ><span class="toc-text">提出问题</span></a
                >
              </li>
              <li class="toc-item toc-level-3">
                <a
                  class="toc-link"
                  href="#%E4%BD%BF%E7%94%A8%E6%95%B0%E5%AD%A6%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0%E9%97%AE%E9%A2%98"
                  ><span class="toc-text">使用数学语言描述问题</span></a
                >
              </li>
              <li class="toc-item toc-level-3">
                <a
                  class="toc-link"
                  href="#%E5%BC%95%E5%85%A5%E5%90%91%E9%87%8F%E8%BF%9B%E8%A1%8C%E8%A1%A8%E8%BE%BE"
                  ><span class="toc-text">引入向量进行表达</span></a
                >
              </li>
            </ol>
          </li>
          <li class="toc-item toc-level-2">
            <a
              class="toc-link"
              href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98"
              ><span class="toc-text">进一步分析问题</span></a
            >
            <ol class="toc-child">
              <li class="toc-item toc-level-3">
                <a
                  class="toc-link"
                  href="#%E5%BC%95%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E8%AE%AD%E7%BB%83%E9%9B%86%E7%9A%84%E6%A6%82%E5%BF%B5"
                  ><span class="toc-text">引入数据集与训练集的概念</span></a
                >
              </li>
              <li class="toc-item toc-level-3">
                <a
                  class="toc-link"
                  href="#%E5%BC%95%E5%85%A5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E8%BD%AC%E5%8C%96%E9%97%AE%E9%A2%98"
                  ><span class="toc-text">引入最小二乘法转化问题</span></a
                >
              </li>
              <li class="toc-item toc-level-3">
                <a
                  class="toc-link"
                  href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"
                  ><span class="toc-text">为什么使用最小二乘法</span></a
                >
              </li>
            </ol>
          </li>
          <li class="toc-item toc-level-2">
            <a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98"
              ><span class="toc-text">解决问题</span></a
            >
            <ol class="toc-child">
              <li class="toc-item toc-level-3">
                <a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"
                  ><span class="toc-text">梯度下降</span></a
                >
              </li>
              <li class="toc-item toc-level-3">
                <a class="toc-link" href="#%E7%89%9B%E9%A1%BF%E6%B3%95"
                  ><span class="toc-text">牛顿法</span></a
                >
              </li>
              <li class="toc-item toc-level-3">
                <a class="toc-link" href="#%E8%A7%A3%E6%9E%90%E8%A7%A3%E6%B3%95"
                  ><span class="toc-text">解析解法</span></a
                >
                <ol class="toc-child">
                  <li class="toc-item toc-level-4">
                    <a
                      class="toc-link"
                      href="#%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89"
                      ><span class="toc-text">符号定义</span></a
                    >
                    <ol class="toc-child">
                      <li class="toc-item toc-level-5">
                        <a
                          class="toc-link"
                          href="#%E5%AE%9A%E4%B9%89%E7%9F%A9%E9%98%B5%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E5%87%BD%E6%95%B0"
                          ><span class="toc-text">定义矩阵函数的导函数</span></a
                        >
                      </li>
                      <li class="toc-item toc-level-5">
                        <a
                          class="toc-link"
                          href="#%E5%AE%9A%E4%B9%89-trace-%E8%BF%90%E7%AE%97"
                          ><span class="toc-text">定义 trace 运算</span></a
                        >
                      </li>
                    </ol>
                  </li>
                  <li class="toc-item toc-level-4">
                    <a
                      class="toc-link"
                      href="#%E4%B8%80%E4%BA%9B%E5%AE%9A%E7%90%86"
                      ><span class="toc-text">一些定理</span></a
                    >
                  </li>
                  <li class="toc-item toc-level-4">
                    <a
                      class="toc-link"
                      href="#%E8%A7%A3%E6%9E%90%E5%BC%8F%E6%8E%A8%E5%AF%BC"
                      ><span class="toc-text">解析式推导</span></a
                    >
                  </li>
                </ol>
              </li>
            </ol>
          </li>
        </ol>
      </div>
      <article class="article-container">
        <h1>线性回归</h1>
        <h2 id="基础">基础</h2>
        <h3 id="定义“特征”与“标签”">定义“特征”与“标签”</h3>
        <p>定义一些概念，方便后面的表达：</p>
        <ul>
          <li>特征，定义为输入空间（特征空间）。</li>
          <li>标签，定义为输出空间。</li>
        </ul>
        <p>对“特征”与“标签”的理解：</p>
        <ol>
          <li>
            <p>从算法定义上：</p>
            <ul>
              <li>特征可理解为输入。</li>
              <li>标签可理解为输出。</li>
              <li>算法的目的就是根据输入，给出输出。</li>
            </ul>
          </li>
          <li>
            <p>从直观感觉上：</p>
            <ul>
              <li>特征指代一个对象的特点。</li>
              <li>标签指代一个对象。</li>
              <li>算法的目的即根据特点，猜测对象。</li>
            </ul>
          </li>
        </ol>
        <h3 id="定义“特征-标签”关系">定义“特征-标签”关系</h3>
        <p>
          实际生活中，我们知道了$ A $，$ B $能够在某种程度上决定$ C $， 且$ A
          $，$ B $各自对$ C $的影响有多大，我们并不清楚。
          也就是说，我们知道“特征”与“标签”之间存在某种关系，但无法准确地描述这个关系。
          此时我们可以对$ A $，$ B $，$ C $建立一个关系，即：能由$ A $，$ B
          $推出$ C $。 使用数学符号表示：$ (A, B) \Rightarrow C $
        </p>
        <p>
          若将$ A $和$ B $称做$ C $的特征，并且将$ C $定义为标签， 将$ (A, B)
          \Rightarrow C $称为“特征-标签”关系， 则称数据$ (A, B, C)
          $存在“特征-标签”关系
        </p>
        <h3 id="提出问题">提出问题</h3>
        <p>
          我们希望，给出一组存在“特征-标签”的关系的数据，
          计算机能够根据这组数据，建立一个存在“特征-标签”关系的模型,
          随后给出“特征”，计算机能够根据模型，给出比较可靠的、推测的“标签”。
        </p>
        <h3 id="使用数学语言描述问题">使用数学语言描述问题</h3>
        <p>
          接下来将问题抽象成为一个数学问题。 设所有的特征的集合为$ (x*{1},
          x*{2}, \cdots, x_{n}) $，其中$ n $表示特征的数量， $ y
          $为特征对应的“标签”。
          在比较简单的情况下，假定“特征-标签”是线性关系，并且这个线性关系为函数$
          h $， 则称$ h $为<strong>假设函数（hypotheses）</strong>。
          于是得到的函数：
        </p>
        <p>
          $$ \begin{equation} h_{\theta}(x) = \theta_{0} + \theta_{1} \cdot
          x_{1} + \theta_{2} \cdot x_{2} + \cdots + \theta_{n} \cdot x_{n}
          \end{equation} $$
        </p>
        <p>
          其中，$ \theta*{0} $，$ \theta*{1} $，$ \cdots $，$ \theta_{n} $
          表示<strong>参数（parameters）</strong>，或者说是<strong>权值（weights）</strong>,
          这是我们需要求得的值。
        </p>
        <h3 id="引入向量进行表达">引入向量进行表达</h3>
        <p>若用向量表示$ \theta $：</p>
        <p>
          $$ \vec{\theta} = \begin{pmatrix} \theta_{0}\\ \theta_{1}\\ \vdots\\
          \theta_{n} \end{pmatrix} $$
        </p>
        <p>并将特征集合重新定义为特征向量：</p>
        <p>
          $$ \vec{x} = \begin{pmatrix} x_{0}\\ x_{1}\\ \vdots\\ x_{n}
          \end{pmatrix} $$
        </p>
        <p>
          其中$ x_{0} = 1 $， **为简化符号，记$ \vec{\theta} $为$ \theta $，$
          \vec{x} $为$ x $**， 则函数可表示为：
        </p>
        <p>
          $$ \begin{equation} h_{\theta}(x) = \sum_{i=0}^{n}{\theta_{i} x_{i}} =
          \theta^{T}x \end{equation} $$
        </p>
        <p>
          此时我们的问题就是，求出一个$ \theta $，使得函数$ h
          $能够很好地描述“特征-标签”关系， 即建立一个合理的模型。
        </p>
        <h2 id="进一步分析问题">进一步分析问题</h2>
        <h3 id="引入数据集与训练集的概念">引入数据集与训练集的概念</h3>
        <p>
          上面的函数仅表示单个数据中的“特征与标签”关系，
          若存在多个数据，即<strong>数据集（dataset）</strong>，
          且所有的数据都存在“特征-标签”的关系， 即可以构建出一个<strong
            >训练集（training set）</strong
          >，
          那么我们可以使用<strong>训练集</strong>对算法进行<strong>训练</strong>，最终得出$
          \theta $。
        </p>
        <p>定义下面的符号，方便之后的表示：</p>
        <p>
          设训练集大小为$ m $，令$ x*{i}^{(j)} $表示训练集中第$ j $项数据的第$ i
          $个特征， $ x^{(j)} $表示训练集中第$ j $项数据的特征向量。 同理可得$
          \theta*{i}^{(j)} $与$ \theta^{(j)} $。
        </p>
        <h3 id="引入最小二乘法转化问题">引入最小二乘法转化问题</h3>
        <p>
          回顾我们的问题： 我们希望得到一个函数$ h
          $能够很好地描述训练集中的每个数据中“特征-标签”的关系
          换句话说，给出“特征”，若“特征”存在于训练集中，那么 由**函数$ h
          $<strong
            >得出的“标签”，与训练集中的“标签”，它们之间的误差最小 根据</strong
          >最小二乘法**，得：
        </p>
        <p>
          $$ \begin{equation} J(\theta) = \frac{1}{2} \sum_{i=1}^{m} \left(
          h_{\theta}(x^{(i)}) - y^{(i)} \right)^{2} \end{equation} $$
        </p>
        <p>函数$ J $也称为<strong>cost function</strong></p>
        <p>
          于是问题转化为： 需要得到一个$ \theta $，使得$ J(\theta) $的值最小
        </p>
        <h3 id="为什么使用最小二乘法">为什么使用最小二乘法</h3>
        <p>
          参考文章
          <a
            href="/2021/11/04/why-use-least-squares/"
            title="为什么使用最小二乘法"
            >为什么使用最小二乘法</a
          >
        </p>
        <h2 id="解决问题">解决问题</h2>
        <p>接下来介绍解决这个问题的可行方法</p>
        <h3 id="梯度下降">梯度下降</h3>
        <blockquote>
          <p>
            注：梯度下降算法在
            <a href="/2021/09/16/gradient-descent/" title="梯度下降算法"
              >另一篇文章</a
            >
            中有介绍
          </p>
        </blockquote>
        <p>
          为了得到$ \theta $使函数值$ J(\theta) $最小，可以采取**梯度下降
          (gradient descent)**的方法： 随机选取一个$ \theta $值，随后不断地更新$
          \theta $，使$ J(\theta) $不断减小， 当$ J(\theta)
          $的值达到最小时，得到的$ \theta $即是问题的解
        </p>
        <p>我们可以这样进行更新：</p>
        <p>
          $$ \begin{equation} \theta_{j} := \theta_{j} - \alpha \frac
          {\partial}{\partial \theta_{j}} J(\theta) \label{eq4} \end{equation}
          $$
        </p>
        <p>
          其中$ := $表示赋值，将右边的值赋予左边，而$ =
          $是真值判断，表示等号两边的值或结果是相同的 $ \alpha $表示<strong
            >学习率 (learning rate)</strong
          >，或者称<strong>步长</strong>，是自定义的常数
        </p>
        <p>对$ J(\theta) $进行偏导：</p>
        <p>
          $$ \begin{equation} \begin{split} \frac{\partial}{\partial \theta_{j}}
          J(\theta) &amp;= \frac{\partial}{\partial \theta_{j}} \frac {1}{2}
          (h_{\theta}(x) - y)^2 \\ &amp;= 2 \cdot \frac{1}{2}(h_{\theta}(x) - y)
          \cdot \frac{\partial}{\partial \theta_{j}} (h_{\theta}(x) - y) \\
          &amp;= (h_{\theta}(x) - y) \cdot \frac{\partial}{\partial \theta_{j}}
          \left( \sum_{i=0}^{n} \theta_{i} x_{i} - y \right) \\ &amp;=
          (h_{\theta}(x) - y) x_{j} \end{split} \end{equation} $$
        </p>
        <p>代入$ \eqref{eq4} $，得：</p>
        <p>
          $$ \begin{equation} \theta_{j} := \theta_{j} - \alpha (h_{\theta}(x) -
          y) x_{j} \end{equation} $$
        </p>
        <p>这是单个数据的表示，引入上面介绍的训练集</p>
        <p>
          $$ \begin{equation} \theta_{j} := \theta_{j} - \alpha \sum_{i=1}^{m}
          \left(h_{\theta}(x^{(i)}) - y^{(i)}\right) x_{j}^{(i)} \end{equation}
          $$
        </p>
        <p>
          对每一个在向量$ \theta $中的分量$ \theta_{j} $都进行上述运算
          最终我们会得到一个$ \theta $，这个值就是我们希望求得的值
        </p>
        <h3 id="牛顿法">牛顿法</h3>
        <blockquote>
          <p>尚未完成，咕咕咕ing</p>
        </blockquote>
        <p>
          因为极值点处的一阶导数为 0 ， 且$ J(\theta)
          $的函数最高次为二次，函数只存在一个极值点 所以，为了得到$ J(\theta)
          $的最小值，我们可以使用间接的方法， 即：寻找$ J(\theta) $导数的零点
        </p>
        <p>牛顿法的步骤为：</p>
        <blockquote>
          <p>设函数为$ f(x) $</p>
          <ol>
            <li>随机在函数$ f(x) $上取一初始点$ P_{0} $</li>
            <li>过点$ P_{0} $作函数$ f(x) $的切线$ L $</li>
            <li>找到切线$ L $与$ x $轴的交点$ Q_{0} $</li>
            <li>过交点$ Q*{0} $作$ x $轴的垂线，并与函数有一交点$ P*{1} $</li>
            <li>将点$ P_{1} $作为初始点，回到步骤 2</li>
            <li>当点$ P*{n} $与点$ Q*{n} $重合时，横坐标即零点</li>
          </ol>
        </blockquote>
        <h3 id="解析解法">解析解法</h3>
        <p>
          实际上，上面的两种方法得到的是一个近似值，
          但其精确度能够满足实际应用， 而解析解法能够直接计算出精确值
        </p>
        <h4 id="符号定义">符号定义</h4>
        <blockquote>
          <p>这里的定义均来自 Andrew NG 课程讲义</p>
        </blockquote>
        <h5 id="定义矩阵函数的导函数">定义矩阵函数的导函数</h5>
        <p>
          对于函数$ f $:$ \mathbb{R}^{m \times n} \mapsto \mathbb{R} $， 即$ m
          \times n $的矩阵映射至实数的函数关系 定义函数$ f $对矩阵$ A
          $的导数为：
        </p>
        <p>
          $$ \begin{equation} \nabla_{A}f(A) = \begin{bmatrix}
          \frac{\partial{f}}{\partial A_{11}} &amp; \cdots &amp;
          \frac{\partial{f}}{\partial A_{1n}} \\ \vdots &amp; \ddots &amp;
          \vdots \\ \frac{\partial{f}}{\partial A_{m1}} &amp; \cdots &amp;
          \frac{\partial{f}}{\partial A_{mn}} \\ \end{bmatrix} \end{equation} $$
        </p>
        <p>
          <em>例子</em> 若矩阵 $ A = \begin{bmatrix} A_{11} &amp; A_{12} \\
          A_{21} &amp; A_{22} \end{bmatrix} $， 函数$ f(A) = \frac{3}{2} A*{11}
          + 5A*{12}^{2} + A*{21}A*{22} $， 那么 $ \nabla*{A} f(A) =
          \begin{bmatrix} \frac{3}{2} &amp; 10A*{12} \\ A*{22} &amp; A*{21}
          \end{bmatrix} $
        </p>
        <h5 id="定义-trace-运算">定义 trace 运算</h5>
        <p>
          <code>trace</code> 运算简写为 <code>tr</code> 对于$ n \times n
          $的矩阵$ A $，定义
        </p>
        <p>
          $$ \begin{equation} \mathrm{tr}A = \sum_{i=1}^{n}A_{ii} \end{equation}
          $$
        </p>
        <p>可以知道， <code>trace</code> 运算具有下列运算法则</p>
        <p>
          $$ \begin{equation} \label{algorithm1} \mathrm{tr}ABC = \mathrm{tr}CAB
          = \mathrm{tr}BCA \end{equation} $$
        </p>
        <p>
          $$ \begin{equation} \label{algorithm2} \mathrm{tr}(A+B) = \mathrm{tr}A
          + \mathrm{tr}B \end{equation} $$
        </p>
        <p>
          $$ \begin{equation} \label{algorithm3} \mathrm{tr}aA = a\mathrm{tr}A
          \end{equation} $$
        </p>
        <p>其中$ a $是一个实数</p>
        <h4 id="一些定理">一些定理</h4>
        <p>
          一些<strong>trace 运算</strong
          >与<strong>矩阵导函数</strong>相关的定理，
          仅列出<strong>解析式推导过程</strong>需要用到的一些定理，不作证明，
        </p>
        <p>
          $$ \begin{equation} \label{theorem1} \nabla_{A} \mathrm{tr}AB = B^{T}
          \end{equation} $$
        </p>
        <p>
          $$ \begin{equation} \label{theorem2} \nabla_{A^{T}} f(A) =
          (\nabla_{A}f(A))^{T} \end{equation} $$
        </p>
        <p>
          $$ \begin{equation} \label{theorem3} \nabla_{A} \mathrm{tr} A B A^{T}
          C = C A B + C^{T} A B^{T} \end{equation} $$
        </p>
        <p>
          $$ \begin{equation} \label{theorem4} \nabla_{A} |A| = |A| ( A^{-1}
          )^{T} \end{equation} $$
        </p>
        <h4 id="解析式推导">解析式推导</h4>
        <p>使用矩阵表达“特征”与“标签” 定义矩阵$ X $和$ Y $</p>
        <p>
          $$ \begin{equation} X = \begin{bmatrix} ( x^{(1)} )^{T} \\ ( x^{(2)}
          )^{T} \\ \vdots \\ ( x^{(m)} )^{T} \end{bmatrix} , Y = \begin{bmatrix}
          y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \end{bmatrix} \end{equation}
          $$
        </p>
        <blockquote>
          <p>注意，此处$ x $表示向量$ \vec{x} $，$ y $表示特征</p>
        </blockquote>
        <p>
          因为$ h_{\theta}(x^{(i)}) = \theta^{T} x^{(i)} = ( x^{(i)} )^{T}
          \theta $，故
        </p>
        <p>
          $$ \begin{equation} h_{\theta}(x)= \begin{bmatrix} ( x^{(1)} )^{T}
          \theta \\ ( x^{(2)} )^{T} \theta \\ \vdots \\ ( x^{(m)} )^{T} \theta
          \end{bmatrix} = X \theta \end{equation} $$
        </p>
        <p>又因为</p>
        <p>
          $$ \begin{equation} X \theta - Y = \begin{bmatrix} ( x^{(1)} )^{T}
          \theta \\ ( x^{(2)} )^{T} \theta \\ \vdots \\ ( x^{(m)} )^{T} \theta
          \end{bmatrix} - \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\
          y^{(m)} \end{bmatrix} = \begin{bmatrix} h_{\theta}(x^{(1)}) - y^{(1)}
          \\ h_{\theta}(x^{(2)}) - y^{(2)} \\ \vdots \\ h_{\theta}(x^{(m)}) -
          y^{(m)} \end{bmatrix} \end{equation} $$
        </p>
        <p>且对于矩阵$ z $，有$ z^{T}z = \sum*{i} z*{i}^{2} $， 于是可得</p>
        <p>
          $$ \begin{equation} \begin{split} \frac{1}{2} (X \theta - Y)^{T} (X
          \theta - Y) &amp;= \frac{1}{2} \sum_{i = 1}^{m} ( h_{\theta}(x^{(i)})
          - y^{(i)} )^{2} \\ &amp;= J(\theta) \end{split} \end{equation} $$
        </p>
        <p>根据$ \eqref{theorem2} $和$ \eqref{theorem3} $，有</p>
        <p>
          $$ \begin{equation} \label{inference1} \nabla_{A^{T}} \mathrm{tr} AB
          A^{T} C = B^{T} A^{T} C^{T} + B A^{T} C \end{equation} $$
        </p>
        <p>于是</p>
        <p>
          $$ \begin{equation} \begin{split} \nabla_{\theta} J(\theta) &amp;=
          \nabla_{\theta} \frac{1}{2} ( X \theta - Y )^{T} (X \theta - Y) \\
          &amp;= \frac{1}{2} \nabla_{\theta} ( \theta^{T} X^{T} X \theta -
          \theta^{T} X^{T} Y - Y^{T} X \theta + Y^{T} Y ) \\ &amp;= \frac{1}{2}
          \nabla_{\theta} \mathrm{tr} ( \theta^{T} X^{T} X \theta - \theta^{T}
          X^{T} Y - Y^{T} X \theta + Y^{T} Y ) \\ &amp;= \frac{1}{2}
          \nabla_{\theta} ( \mathrm{tr} \theta^{T} X^{T} X \theta - 2
          \mathrm{tr} Y^{T} X \theta ) \\ &amp;= \frac{1}{2} ( X^{T} X \theta +
          X^{T} X \theta - 2 X^{T} Y ) \\ &amp;= X^{T} X \theta - X^{T} Y
          \end{split} \end{equation} $$
        </p>
        <p>
          第三步将 $(\theta^{T} X^{T} X \theta - \theta^{T} X^{T} Y - Y^{T} X
          \theta + Y^{T} Y )$ 视作整体， 可知这是一个实数，可将其看作一个 $ 1
          \times 1 $的矩阵$ A = \begin{bmatrix}a\end{bmatrix} $， 由
          <code>trace</code> 定义可知$ \mathrm{tr} A = a $
        </p>
        <p>
          第四步使用了$ \mathrm{tr} A = \mathrm{tr} A^{T} $， 且由于$ Y^{T}Y
          $项不存在$ \theta $，求导后为$ 0 $
        </p>
        <p>
          第五步中使用了$ \eqref{theorem1} $和$ \eqref{inference1} $， 视$ A^{T}
          = \theta $，$ B = B^{T} = X^{T} X $,$ C = I $
        </p>
        <p>
          使$ J(\theta) $最小，则令$ \nabla_{\theta} J(\theta) = 0 $， 所以有$
          X^{T} X \theta = X^{T} Y $ 因此，得$ \theta = ( X^{T}X )^{-1} X^{T} Y
          $
        </p>
        <hr class="footnotes-sep" />
        <section class="footnotes">
          <ol class="footnotes-list">
            <li id="fn1" class="footnote-item">
              <p>
                课程链接
                <a href="https://see.stanford.edu/Course/CS229"
                  >https://see.stanford.edu/Course/CS229</a
                >
                <a href="#fnref1" class="footnote-backref">↩︎</a>
              </p>
            </li>
          </ol>
        </section>
        <hr />
        <div class="article-meta-info">
          <span>分类</span>
          <a
            class="article-categories-link"
            href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"
            >机器学习</a
          >
          <span>标签</span>
          <a
            class="article-tags-none-link"
            href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"
            rel="tag"
            >监督学习</a
          ><a
            class="article-tags-none-link"
            href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"
            rel="tag"
            >线性回归</a
          >
        </div>
        <div class="article-date-info">
          <span>发布日期: 2021-09-11</span>
          <span>更新日期: 2023-07-21</span>
        </div>
        <div class="article-share-link">
          <span>
            分享文章
            <a href="https://huzerovo.github.io/2021/09/11/linear-regression/"
              >https://huzerovo.github.io/2021/09/11/linear-regression/</a
            >
          </span>
        </div>
        <div class="article-comment">
          <div id="gitalk-container"></div>
          <link
            rel="stylesheet"
            href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"
          />
          <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
          <script src="/js/md5-2.19.0.min.js"></script>
          <script>
            let c = {
              clientID: "8430fc1311285afb9029",
              clientSecret: "30d89bdade46310cbbb97ca58bdd6f615ddcd699",
              repo: "huzerovo.github.io",
              owner: "Huzerovo",
              admin: ["Huzerovo"],
              distractionFreeMode: false,
              number: -1,
              title: "Article: 线性回归",
            };
            c.id = md5(c.title);
            let gitalk = new Gitalk(c);
            gitalk.render("gitalk-container");
          </script>
        </div>
      </article>
    </main>
    <button id="goto-top" type="button" title="回到顶部">回到顶部</button>
    <footer id="site-footer">
      <p>© 2025 @狐zerOAO</p>
      <p>
        如无特殊说明，网站所有原创内容均采用<a
          href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans"
          >CC BY-NC-SA 4.0</a
        >协议授权。
      </p>
      <p>
        Powered by <a href="https://hexo.io">Hexo</a>. Theme by
        <a href="https://github.com/Huzerovo/hexo-theme-huzerovo">狐zerOAO</a>.
      </p>
      <br />
    </footer>
  </body>
  <script>
    hljs.highlightAll();
    $("#goto-top").hide();
    // 页面载入后设置icon
    ThemeSwitcher.follow();
    $(document).ready(function () {
      // 监听事件
      $("#btn-switch-theme").click(switchThemeClickHandler);
      // $("#btn-side-menu").click(function () {
      //   let e = $("#side-nav");
      //   let hide = "side-menu-hidden";
      //   let show = "side-menu-show";
      //   if (e.hasClass(hide)) {
      //     e.removeClass(hide);
      //     e.addClass(show);
      //   } else {
      //     e.removeClass(show);
      //     e.addClass(hide);
      //   }
      // });
      $("#search-input").on("input", function () {
        searchArticle("/search.xml");
      });
      $("#goto-top").click(function () {
        $("#root").scrollTop(0);
      });
      $(window).scroll(function () {
        if ($("#root").scrollTop() > 300) {
          $("#goto-top").show();
        } else {
          $("#goto-top").hide();
        }
      });
      Fancybox.bind(".container img");
      Fancybox.bind("[data-fancybox]");
    });
  </script>
</html>
